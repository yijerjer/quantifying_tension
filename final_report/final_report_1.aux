\relax 
\citation{Franklin2013}
\citation{deVaucouleurs1986,Sandage1975}
\citation{Planck2020,Abbott2018,Freedman2020,Riess2019,Wong2019}
\citation{DiValentino2021}
\citation{Planck2020,Abbott2018}
\citation{Freedman2020,Riess2019,Wong2019}
\citation{Planck2020,Abbott2018}
\citation{Freedman2020,Riess2019,Wong2019}
\citation{Heymans2021}
\citation{Handley2021Closed}
\citation{Handley2019}
\citation{Charnock2017}
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Constructing Non-Linear Cosmological Tension Coordinate with Neural Networks}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}}
\newlabel{intro}{{I}{1}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A compilation of recent measurements of the Hubble constant $H_0$. The top two measurements are from early-universe datasets using $\Lambda $CDM cosmology \cite  {Planck2020, Abbott2018}, while the remaining three are from late-universe datasets based off local distance ladder measurements \cite  {Freedman2020, Riess2019, Wong2019}. The tension between the \textit  {Planck} and SH0ES measurements currently stands at $4.7 \sigma $.}}{1}{}}
\newlabel{H0_tension}{{1}{1}{}{}{}}
\citation{Trotta2008}
\citation{Marshall2006,Lemos2020_1}
\citation{Handley2019}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The main plot shows a two-dimensional marginalised distribution of the \textit  {Planck} and DES datasets in the $\Omega _m$ - $\sigma _8$ plane, where the solid contours contain 68\% and 95\% of the marginalised probability mass. The two smaller plots are the one-dimensional marginalised distributions of both datasets in each parameter.}}{2}{}}
\newlabel{omegam_sigma8}{{2}{2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Bayesian statistics}{2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Bayesian evidence}{2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Bayes factor $R$}{2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Tension coordinate}{2}{}}
\citation{Handley2019}
\citation{Hornik1989}
\citation{Nwankpa2018}
\citation{Nair2010}
\citation{Keskar2017}
\citation{Kingma2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Gaussian example}{3}{}}
\newlabel{gaussian_tension}{{II\,C\,1}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Non-Gaussian case}{3}{}}
\newlabel{non_gaussian}{{II\,C\,2}{3}{}{}{}}
\newlabel{margin_R}{{11}{3}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Neural network and training}{3}{}}
\citation{Rosenblatt1956,Silverman1986}
\citation{Rosenblatt1956}
\citation{Turlach1993}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A fully-connected neural network with 6 nodes in the input layer, an arbitrary number of nodes in the hidden layer, and a single node in the output layer.}}{4}{}}
\newlabel{fig:NN}{{3}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Numerical calculation of Bayes factor}{4}{}}
\newlabel{section:numerical_bf}{{III\,B}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Toy examples}{4}{}}
\citation{Handley2019,Lemos2020}
\citation{Dataset}
\citation{anesthetic}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Training of Neural Network}}{5}{}}
\newlabel{algo}{{1}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Cosmological dataset}{5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Identifying the source(s) of tension}{5}{}}
\newlabel{section:source}{{III\,E}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Six parameters}{5}{}}
\citation{Handley2021}
\citation{Handley2019}
\citation{Handley2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The main contour plots illustrate the iso-tension coordinate hypersurfaces shaped by the neural network after training for 500 epochs on our toy examples. a) shows two Gaussian distributions and b) shows a Gaussian distribution accompanied by a 'banana-shaped' distribution. The faded blue distribution in the background is the prior. The smaller plots along the $x$ and $y$ axes show the marginalised distributions of these toy examples in their respective axes.}}{6}{}}
\newlabel{fig:toy}{{4}{6}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Pair-wise comparison}{6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Standardising the tension coordinate}{6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G}Computing a more interpretable tension}{6}{}}
\citation{Handley2019}
\citation{Dataset}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The quantities related to the six-parameter marginalised tension obtained using two methods -- the Gaussian assumption detailed in Section II\,C\,1{}{}{}\hbox {}, and our neural network approach. There are no errors for the Gaussian assumption method because the tension coordinate can be constructed analytically.}}{7}{}}
\newlabel{table:six_params}{{I}{7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results and Discussion}{7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Six parameters}{7}{}}
\newlabel{section:six}{{IV\,A}{7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density plots of the marginalised posteriors of the DES and Planck posteriors and the prior, in the 1D tension coordinate. (a) uses the Gaussian assumption described in Section II\,C\,1{}{}{}\hbox {} and (b) uses the neural network approach.}}{7}{}}
\newlabel{fig:six_compare}{{5}{7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Measured absolute gradient of the tension coordinate $t$ with respect to each parameter across all data points of both datasets, provided alongside the standard deviation of the measurements}}{8}{}}
\newlabel{fig:grad}{{6}{8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Pair-wise comparison}{8}{}}
\newlabel{section:pair}{{IV\,B}{8}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusions}{8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The background of these plots shows the two-dimensional marginalised posteriors of the DES and \textit  {Planck} posteriors and their prior. The foreground in these plots, bar the last row, illustrates contour plots of the iso-tension coordinate hypersurfaces. The bold contour line is the midpoint between the two datasets in the 1D tension coordinate. When drawing these 2D contour plots, the four non-participating parameters are fixed at their respective means in the joint DES-\textit  {Planck} dataset. The final row plots the te;nsion coordinate $t$ against the six cosmological parameters, visually emphasising the maximum tension found between these two datasets. The marginalised Bayes factor for this particular optimised neural network is computed to be $-16.9$. \\ \\}}{9}{}}
\newlabel{fig:six}{{7}{9}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Similar to Figure 7{}{}{}\hbox {}, the background shows 2D marginalised distributions of the two datasets and the prior, and the foreground illustrates iso-tension coordinate contour plots. The key difference in this figure is that each plot represents an independent neural network with the respective parameter pair as its input, unlike Figure 7{}{}{}\hbox {} where all the plots collectively represent a single neural network with six input parameters. Note that the contour plots here are much noisier, particularly in regions that are further away from the two datasets. This likely points to an oversized hidden layer of the neural network, where it begins to over-fit to the statistical noise of the two datasets. \\ \\}}{10}{}}
\newlabel{fig:pairs}{{8}{10}{}{}{}}
\citation{Udrescu2020}
\citation{Montavon2018}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The upper table details the quantities of tension calculated from trained neural networks with parameter pairs as its input. The lower table gives the mean of the quantities of tension from the upper table for each parameter, where the mean is over the pairs which contain the respective parameter. Both tables are ordered in decreasing tension.}}{11}{}}
\newlabel{table:pairwise}{{II}{11}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Further Work}{11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgments}{11}{}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Standardising the tension coordinate}{11}{}}
\newlabel{appendix:standardising}{{A}{11}{}{}{}}
\citation{Kullback1951}
\bibdata{final_report_1Notes,apssamp}
\bibcite{Franklin2013}{{1}{2013}{{Franklin}}{{}}}
\bibcite{deVaucouleurs1986}{{2}{1986}{{de~Vaucouleurs}}{{}}}
\bibcite{Sandage1975}{{3}{1975}{{Sandage\ and\ Tammann}}{{}}}
\bibcite{Planck2020}{{4}{2020}{{Aghanim\ \emph  {et~al.}}}{{Aghanim, Akrami, Ashdown, Aumont, Baccigalupi, Ballardini, Banday, Barreiro, Bartolo,\ and\ et~al.}}}
\bibcite{Abbott2018}{{5}{2018}{{Abbott\ \emph  {et~al.}}}{{Abbott, Abdalla, Annis, Bechtol, Blazek, Benson, Bernstein, Bernstein, Bertin, Brooks,\ and\ et~al.}}}
\bibcite{Freedman2020}{{6}{2020}{{Freedman\ \emph  {et~al.}}}{{Freedman, Madore, Hoyt, Jang, Beaton, Lee, Monson, Neeley,\ and\ Rich}}}
\bibcite{Riess2019}{{7}{2019}{{Riess\ \emph  {et~al.}}}{{Riess, Casertano, Yuan, Macri,\ and\ Scolnic}}}
\bibcite{Wong2019}{{8}{2019}{{Wong\ \emph  {et~al.}}}{{Wong, Suyu, Chen, Rusu, Millon,\ and\ et~al.}}}
\bibcite{DiValentino2021}{{9}{2021}{{Valentino\ \emph  {et~al.}}}{{Valentino, Mena, Pan, Visinelli, Yang, Melchiorri, Mota, Riess,\ and\ Silk}}}
\bibcite{Heymans2021}{{10}{2021}{{Heymans\ \emph  {et~al.}}}{{Heymans, Tröster, Asgari, Blake, Hildebrandt, Joachimi, Kuijken, Lin, Sánchez, van~den Busch,\ and\ et~al.}}}
\bibcite{Handley2021Closed}{{11}{2021}{{Handley}}{{}}}
\bibcite{Handley2019}{{12}{2019}{{Handley\ and\ Lemos}}{{}}}
\bibcite{Charnock2017}{{13}{2017}{{Charnock\ \emph  {et~al.}}}{{Charnock, Battye,\ and\ Moss}}}
\bibcite{Trotta2008}{{14}{2008}{{Trotta}}{{}}}
\bibcite{Marshall2006}{{15}{2006}{{Marshall\ \emph  {et~al.}}}{{Marshall, Rajguru,\ and\ Slosar}}}
\bibcite{Lemos2020_1}{{16}{2020{}}{{Lemos\ \emph  {et~al.}}}{{Lemos, Köhlinger, Handley, Joachimi, Whiteway,\ and\ Lahav}}}
\bibcite{Hornik1989}{{17}{1989}{{Hornik\ \emph  {et~al.}}}{{Hornik, Stinchcombe,\ and\ White}}}
\bibcite{Nwankpa2018}{{18}{2018}{{Nwankpa\ \emph  {et~al.}}}{{Nwankpa, Ijomah, Gachagan,\ and\ Marshall}}}
\bibcite{Nair2010}{{19}{2010}{{Nair\ and\ Hinton}}{{}}}
\bibcite{Keskar2017}{{20}{2017}{{Keskar\ \emph  {et~al.}}}{{Keskar, Mudigere, Nocedal, Smelyanskiy,\ and\ Tang}}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Kullback-Leibler divergence}{12}{}}
\newlabel{appendix:kl_divergence}{{B}{12}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{12}{}}
\bibcite{Kingma2017}{{21}{2017}{{Kingma\ and\ Ba}}{{}}}
\bibcite{Rosenblatt1956}{{22}{1956}{{Rosenblatt}}{{}}}
\bibcite{Silverman1986}{{23}{1986}{{Silverman}}{{}}}
\bibcite{Turlach1993}{{24}{1993}{{Turlach}}{{}}}
\bibcite{Lemos2020}{{25}{2020{}}{{Lemos\ \emph  {et~al.}}}{{Lemos, Raveri, Campos, Park, Chang, Weaverdyck, Huterer, Liddle, Blazek, Cawthon, Choi,\ and\ et. al}}}
\bibcite{Dataset}{{26}{2020}{{Handley\ and\ Lemos}}{{}}}
\bibcite{anesthetic}{{27}{2019}{{Handley}}{{}}}
\bibcite{Handley2021}{{28}{2021}{{Handley\ and\ Lemos}}{{}}}
\bibcite{Udrescu2020}{{29}{2020}{{Udrescu\ and\ Tegmark}}{{}}}
\bibcite{Montavon2018}{{30}{2018}{{Montavon\ \emph  {et~al.}}}{{Montavon, Samek,\ and\ Müller}}}
\bibcite{Kullback1951}{{31}{1951}{{Kullback\ and\ Leibler}}{{}}}
\bibstyle{apsrev4-2}
\citation{REVTEX42Control}
\citation{apsrev42Control}
\newlabel{LastBibItem}{{31}{13}{}{}{}}
\newlabel{LastPage}{{}{13}{}{}{}}
\gdef \@abspage@last{13}
